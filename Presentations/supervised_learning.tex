% $Header: /cvsroot/latex-beamer/latex-beamer/solutions/generic-talks/generic-ornate-15min-45min.en.tex,v 1.5 2007/01/28 20:48:23 tantau Exp $

\documentclass{beamer}

% This file is a solution template for:

% - Giving a talk on some subject.
% - The talk is between 15min and 45min long.
% - Style is ornate.



% Copyright 2004 by Till Tantau <tantau@users.sourceforge.net>.
%
% In principle, this file can be redistributed and/or modified under
% the terms of the GNU Public License, version 2.
%
% However, this file is supposed to be a template to be modified
% for your own needs. For this reason, if you use this file as a
% template and not specifically distribute it as part of a another
% package/program, I grant the extra permission to freely copy and
% modify this file as you see fit and even to delete this copyright
% notice. 


\mode<presentation>
{
  \usetheme{Warsaw}
  % or ...

  \setbeamercovered{transparent}
  % or whatever (possibly just delete it)
}


\usepackage[english]{babel}
% or whatever

\usepackage[latin1]{inputenc}
% or whatever
\usepackage{amsmath}

\usepackage{times}
\usepackage[T1]{fontenc}
% Or whatever. Note that the encoding and the font should match. If T1
% does not look nice, try deleting the line with the fontenc.


\title{Supervised Learning:} %[Supervised Learning] % (optional, use only with long paper titles)


\subtitle
{Classification in a nutshell} % (optional)

\author[josh bowles] % (optional, use only with lots of authors)
{josh bowles\inst{1,}\inst{2}}
% - Use the \inst{?} command only if the authors have different
%   affiliation.

\institute[University of Utah and One On One Media] % (optional, but mostly needed)
{
  \inst{1}%
  One on One Media\\
  Software Developer
\and
  \inst{2}%
  Department of Philosophy (August 2012),\\
  Experimental Epistemology and Machine Learning\\
  University of Utah 
}
% - Use the \inst command only if there are several affiliations.
% - Keep it simple, no one is interested in your street address.

\date[\Today] % (optional)
{Date / Bayes Group}

\subject{Reverend Bayes}
% This is only inserted into the PDF information catalog. Can be left
% out. 



% If you have a file called "university-logo-filename.xxx", where xxx
% is a graphic format that can be processed by latex or pdflatex,
% resp., then you can add a logo as follows:

\pgfdeclareimage[height=0.9cm]{university-logo}{code-merge}
\logo{\pgfuseimage{university-logo}}



% Delete this, if you do not want the table of contents to pop up at
% the beginning of each subsection:
\AtBeginSubsection[]
{
  \begin{frame}<beamer>{Outline}
    \tableofcontents[currentsection,currentsubsection]
  \end{frame}
}


% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command: 

\beamerdefaultoverlayspecification{<+->}



\AtBeginSubsection[]
{
 \begin{frame}<beamer>{Outline}
   \tableofcontents[currentsubsection]
  \end{frame}
}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Outline}
  \tableofcontents
  %% You might wish to add the option [pausesections]
\end{frame}

\section{What is Supervised Learning?}

\begin{frame}{This picture does not warm your heart}
	\begin{figure}[htpb]
		\begin{center}
			\includegraphics[width=0.7\textwidth,height=0.7\textheight]{mercury1}\caption{\scriptsize Mercury, NASA}
		\end{center}
	\end{figure}
\end{frame}


\begin{frame}{Target (Approximation or Function)}
  Less ``science-fiction'' name would be more appropriate: Target Approximation.
  \begin{exampleblock}{\sc Machine Learning}
		Machine learning is really just a group of algorithms that all have similar end-game strategies: to make predictions given new data based on training data that specifies a target to be optimally approximated... or given the new data we want to (re)define a target function that will predict ``probably approximately correct'' (PAC) values
	\end{exampleblock}
	According to Ai taxonomy:
  \begin{enumerate} 
		\item Fully observable
		\item Stochastic
		\item Continuous
		\item Benign (non-adversarial)
  \end{enumerate}
\end{frame}

\begin{frame}{This picture does not warm your heart either}
	\begin{figure}[htpb]
		\begin{center}
			\includegraphics[width=0.7\textwidth,height=0.7\textheight]{/Users/jbowles/texmf/tex/gix/Von_Neumann_5.jpeg}\caption{\scriptsize State of the Art Computer}
		\end{center}
	\end{figure}
\end{frame}

\begin{frame}{Functions that write functions}
  An idea I am exploring a little more (since this morning!)
  \begin{exampleblock}{Machine Learning and Meta-Programming}
    Machine Learning and Meta-Programming share some basics.
    Except ML attempts to design functions that write functions that perform
    better than their ancestors. Technical details depend on how we define
    ``perform better''. 
  \end{exampleblock}
 MP: Introspection (accessing language constructs at run-time) allows you to update/modify/etc\dots any construct
 at any time.
 ML: Functions that need to update variables given unknown values of those variables, which in turn pipes output
 to another function. ML does this iteratively at massive scale.
\end{frame}

\begin{frame}{Neural Networks}
	\begin{figure}[htpb]
		\begin{center}
			\includegraphics[width=0.7\textwidth,height=0.7\textheight]{/Users/jbowles/texmf/tex/gix/n-net.jpg}\caption{\scriptsize 3 Layer Neural Network, Hidden Layer is Meta-Programming Layer}
		\end{center}
	\end{figure}
\end{frame}

\begin{frame}{Taxonomy}
	Learning via structural inference = 
	\begin{exampleblock}{\sc Not Deductive Logic}
		There is no contextual awareness and no domain-specific knowledge. There is no analogical learning (learning via analogy/example... what humans seem to be best at) that requires contextual-historical knowledge. This should not surprise anyone.
	\end{exampleblock}
	\begin{exampleblock}{from Danks, to appear:4}
		Structural inference uses (relatively) domain-general algorithms whose success depends on the internal structure of the data, rather than features of the semantic content of the data
	\end{exampleblock}
	Danks, to appear. Learning. {\it Cambridge handbook to artificial intelligence}
\end{frame}

\begin{frame}{More definitions}
	Supervised Learning algorithms assume that some variable X is designated as the target for prediction, explanation, or inference, and that the values of X in the dataset constitute the ``ground truth'' values for learning. That is, supervised learning algorithms use the known values of X to determine what should be learned.
\end{frame} 

\begin{frame}{More definitions...cont2}
\begin{exampleblock}{Tom M. Mitchell's widely cited definition}	
	 A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.
\end{exampleblock}
\end{frame}

\begin{frame}{This picture is just freaky}
	\begin{figure}[htpb]
		\begin{center}
			\includegraphics[width=0.7\textwidth,height=0.7\textheight]{inutero}\caption{\scriptsize This has nothing to do with supervised learning}
		\end{center}
	\end{figure}
\end{frame}


\subsection{Classification Tasks}
\begin{frame}{Binary Classification}\label{bin-classification}
{\bf Data:} We need multiple sets of data; minimally, one for training and one for testing. A set of data is composed of, {\bf at least}, the following:
\begin{enumerate}
\item $\mathfrak{n}$ attributes: $[\mathcal{A}_0, \mathcal{A}_2, ..., \mathcal{A}_\mathfrak{n}]$; typically vectorized, such that $\mathfrak{n} \in \mathbb{R}$, or $\mathbb{R}^\mathfrak{n}$
\item a classification class $\mathbb{F}$ (= {\sc financial})
\item a hypothesis space $\mathcal{H}$ (also called a class, but not to be confused with  classification classes), for which each solution is a hypothesis $\mathit{h} \in \mathcal{H}$
\end{enumerate}
\end{frame}

\begin{frame}{Binary Classification cont...2}\label{bin2-classification}
{\bf Goal:} Define a classification model. Since this is binary we only need to define models with two values (positive and negative) for our class: $ { \mathfrak{p}, \mathfrak{n} } \in \mathbb{F}$.

Our goal is: {\sl hypothesis} $\mathit{h} \in \mathcal{H}$ that approximates $\mathbb{F}$ as closely as possible.

\begin{exampleblock}{Supervision}
	All steps are being supervised by us... we are setting the values and determining both the goals and data at every point of the process, including our sense or intuition about how closely our hypothesis approximates the classification class
\end{exampleblock}
\end{frame}

\begin{frame}{Binary Classification cont...3}\label{bin3-classification}
\begin{enumerate}
	\item data set = D
	\item task = T
	\item performance measure = M
\end{enumerate}

Does our program ``learn'' from D to optimize task T given performance measure M? That is, does our program get better as approximating its target or re-defining its function to approximate correctly? 

This is really general.
\end{frame}

\begin{frame}{Continuous Classification}\label{cont-classification}
	
\begin{enumerate}
\item Get
\item To
\item This
\item Later
\end{enumerate}
\end{frame}

\begin{frame}{Google uses various multi-media classification}
	\begin{figure}[htpb]
		\begin{center}
			\includegraphics[width=0.7\textwidth,height=0.7\textheight]{/Users/jbowles/texmf/tex/gix/warm-heart-google}\caption{\scriptsize Text to visual classification uses various continuous classification model}
		\end{center}
	\end{figure}
\end{frame}



\section{What is Learning?}
\subsection{Targets}
\begin{frame}{Learn towards targets}
	The most successful machine learning applications use
  \textbf{Math and Logic and Language} targets that are structural and not determined by context or domain-knowledge.
 
\end{frame}

\subsection{Training}
\begin{frame}{Learn from training}
  If
\end{frame}

\subsection{In Practice}
\begin{frame}{Learn from practice}
  It's not really about the machine learning anything... it is about \textcolor{red}{YOU}.
The machine learns nothing, it's about us learning how to optimize tasks given the limits of our machines.

\begin{exampleblock}{Note on Programming Languages}
	Ruby is great for rapid prototyping. If you want/need a scripting language, Python is still your best bet. Java seems to have the market share of machine learning.
\end{exampleblock}

\end{frame}

\subsubsection{Keyword Classification}
\begin{frame}{Sponsored Search}\label{keyword}
\begin{block}{The Problem}
	Company has a large majority of its ad content and keyword bidding centered on financial aid incentives. It now needs to diversify its message content.
\end{block}

\begin{block}{The Solution}
  Various Classifiers that will approximate the target classification Financial.
\end{block}
\end{frame}

\begin{frame}{Classes}\label{keyword_cont1}
Classification Training
\begin{enumerate}
	\item data\_entities/financial.txt
	\item data\_entities/financial\_not.txt
\end{enumerate}

Data Training
\begin{enumerate}
	\item data\_training/\*
	\item data\_analysis/\*
\end{enumerate}
\end{frame}



% Since this a solution template for a generic talk, very little can
% be said about how it should be structured. However, the talk length
% of between 15min and 45min and the theme suggest that you stick to
% the following rules:  

% - Exactly two or three sections (other than the summary).
% - At *most* three subsections per section.
% - Talk about 30s to 2min per frame. So there should be between about
%   15 and 30 frames, all told.


\end{document}



