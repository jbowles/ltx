
\documentclass[11pt,twoside]{article}

\usepackage{paperp}
\usepackage{paperc}
\usepackage{linguex}

\begin{document}

\title{Ambiguous ($\alpha$)Merge, Symmetry, and Group Theory: A note on the ideal structure of a hypothetical operation}

\author{Joshua Bowles}

\date{\today}
\maketitle

\begin{abstract} This paper is a speculative work. It is concerned with formal foundational issues in syntactic theory. The properties and implications of the operation Merge are reviewed in both a linguistic and computational theory setting. A brief sketch is given for the historical background of formalist thinking from which Merge developed, and the operation ambiguous Merge, here called $\alpha$Merge, is defined in an algebraic group theoretic framework. The idealized and mathematical framework in which $\alpha$Merge is defined is broadened to include current linguistic interest in Fibonacci patterns as well as non-linguistic interest in the Church-Turing Thesis. Within this context, a detailed comparison is given between the output of general phrase structure relations for $\alpha$Merge and for results in \cite{medeiros:2008}. From this, a translational symmetry between constituents at phrase and head levels is observed and defined as the ``X-ratio.'' Next follows the argument that given multiple kinds of symmetry observed in syntax---generally of syntactic objects \textsl{and} rules---it is reasonable to investigate the mathematical study of symmetry---Group Theory---for use in syntax. This paper also proposes a corollary to a weakened version of the Strong Minimalist Thesis and discusses basic assumptions about the use of formal models and computability in the study of natural language---to which research of symmetry in syntax must be accountable. Finally, given a group theoretical definition of the operation $\alpha$Merge, some phase-based implications are presented.  
\end{abstract}
\textbf{Keywords}: Symmetry, Group Theory, Merge, Fibonacci pattern\\




%section INTRODUCTION
\section{Introduction}\label{intro}

Natural language cannot be exhaustively modeled by a formal system such as set theory or first order predicate logic. Early work on language stemming from the ``linguistic turn'' in philosophy of science (e.g., \cite{ayer:1936}, \cite{carnap:1937}, \cite{riechenbach:1947}, \cite{tarski:1956}, \cite{popper:1959}, \cite{quine:1953}) coupled with the structuralist tradition in linguistics from Bloomfield, Sapir, and Harris, quickly showed that an efficient and consistent model of grammar could only be gotten in one of two ways: (a) a synthetic grammar contrived from a first order predicate calculus, or (b) accounting for the distribution of particular morphosyntactic elements in produced language data (see \cite{tomalin:2006} for an account of the formal foundations of modern linguistics). Neither (a) nor (b) shed any explanatory light on what was to quickly become the driving insight of the ``new school'' of linguistics: that any human being of sufficiently young age could acquire any human language with a statistically insignificant measure of data (i.e., the poverty of stimulus).\footnote{These issues are still at the center of research for theoretical syntax, natural language processing, and artificial intelligence.}
 
Eventually the conceptual tensions  between (a) and (b), as well as the vague notions of \emph{competence} and \emph{performance} underlying the ``new school,'' forced a distinction between Internal and External structure known as I-language and E-language. The focus of most generative syntacticians is I-language; their goal is to exhaustively describe and explain the internal mechanisms responsible for initial language acquisition for any and all human languages. In the current climate of Minimalist research it is now more appropriate, following \cite{ppuriagereka:2008}, to talk of the ``growth form'' in terms of acquisition. That is, I-language, as a growth form unique to the human organism, is a natural object. As such, it can be modeled by various formal or mathematical systems with varying degrees of success; relative to specific goals. Here I want to emphasize the point first made in \cite{chomsky55logicalsyntax}, contra \cite{barhillel:1954}, that natural language is not a \emph{formal} object. Natural language is---at a specifically defined level of analysis---an object of nature that is subject to constraints, laws, and tendencies of the patterns of nature; see also \cite{chomsky95langnature} and \cite{boeckxpp:2005}. Consequently, we should not expect any formal/mathematical system to exhaustively model natural language; but useful methods for mathematicization should always motivate us.\footnote{The implication here is very clear, at least to me: linguists will necessarily have to construct the formalisms needed to model natural language capacity. No object or phenomenon in nature is inherently formal (unless one is a strict Platonist), but such natural objects or phenomena are amenable to formal modeling. This modeling may not be precisely exhaustive, it only needs to approximate enough precision to be useful. This is why integral and differential calculus is so important: it allows one to ignore small parts of imprecision that have negligible effects on the outcomes or predictions of one's model. The same can be said, perhaps with more zeal, for probability and statistics. However, recognition of the reality of imprecise and inexhaustive models does not stop one from {\sl attempting} exhaustive precision models. We should assume the same methodology applies to linguistics in general.}

As it stands now, with recent formulations of X-bar theory and interest in Fibonacci (F) patterns arising from ideal iterations in both prosody and phrase structures, a special role for rigorously modeling the computability requirements of producing iterated patterns emerges.\footnote{Not to mention, the majority of linguists, despite theoretical persuasion, would probably agree that natural language exploits iterated patters in general. Whether these patterns emerge as a consequence of the laws of physics and biology (Minimalist syntax/semantics), or from cultural patterns related to general human cognition (pragmatics) is beside the point. Focusing on iterated patterns is a sound methodological strategy that unifies linguists in general.} That is, F patterns produce self-similar structures along repeated and computable iterations---and, computable iterations of the F sequence can produce idealized X-bar structures, as \cite{medeiros:2008} and \cite{soschen:2008} have shown. These idealized X-bar structures also map projections of phase domains that have implicational patterns, as \cite{ppuriagereka:2008} discuss. Because F sequences are computable, iterated, self-similar generations they also produce symmetrical forms. If one takes the F sequence of X-bar structures and compares two different levels, head (X$^{0}$) and phrase (XP), a ratio can be derived that shows symmetry between them; I call this the X-ratio in \S~ \ref{x}. In the context of these developments it is only natural to look towards the formal study of symmetry---Group Theory---for possible rigor. Most exciting, and necessary, is that if one can develop or derive some mathematization for symmetry in syntax, then one can also develop more rigorous notions of symmetry-breaking in syntax (e.g., for the well-founded formulation that verbs project an asymmetrical relation with external arguments, and many others).

In any system that produces hierarchical embedding the key notion is not symmetry itself, but symmetry-breaking. In linguistic terms, a computable operation Merge will combine two objects \{$\alpha$\}, \{$\beta$\} to produce a mirror-symmetric object if not restricted in any way. That is, no matter how you flip the two branches of the tree along a vertical axis, the relation will be the same---with the root node label unspecified. Such an object is ambiguous to the extent that the root node label (i.e., projected bundle of lexical features of one or the other \{$\alpha$\}, \{$\beta$\}, or projected Probe of either \{$\alpha$\}, \{$\beta$\}whose \textsl{u}F gets valued during the merge operation) is ambiguous or unspecified. When no specification for the root node label is given then no hierarchy can be established between the two syntactic objects, as \cite{boeckx08bare} and \cite{chomsky95mp} discuss. If we iterate this process, unrestricted, then projection of label to the root node will be random, i.e., there is no deciding information for which of the two labels, $\alpha$ or $\beta$, projects. Real world syntax necessitates very specific projections---whether they are simple labels, lexical feature bundles, Probe with valued \textsl{u}F, or some other property.\footnote{Of course, adjuncts and adjunction present problems of a different sort: such as a possible immediate asymmetry of Merge, represented by the ordered pair set notation \mbox{$<a, b>$} in contrast to typical set notation \{$a \{a, b$\}\}; see \cite{boeckx08bare}, \cite{chametzky:2000} \cite{hn:2008}, and \cite{rubin:2002}, \cite{rubin:2003} for examples.}  In other words, empirical data show overwhelming evidence that syntactic relations between constituents are not altogether symmetrical and that hierarchical (asymmetric or antisymmetric) relations are a simple fact of human language. The reason for focusing so intently on symmetry, then, is to have some criteria by which to measure symmetry-breaking. Lastly, another benefit of focusing on symmetry is twofold: (i) it has a productive precedence in linguistics with the notion of antisymmetry in \cite{kayne:1994}, and (ii) one need not refer to notions of recursion to talk about symmetry; depending on how this is interpreted, one could say that recursion is byproduct of symmetrical relations. In other words, focusing on symmetry (specifically in a group-theoretic framework) give us recursion for free without having to rely only on recursion. Group theoretic symmetries can be explained or modeled in many different ways and showing that two elements are symmetric does not necessitate a recursive-generative model, though two symmetric elements can certainly be modeled this way. 


%section 1:
\section{$\alpha$Merge and symmetry}
\subsection{Ambiguous Merge}\label{amerge:sub}

Given the naturalistic and rigorous framework for linguistic study briefly discussed in \S~\ref{intro}, it is reasonable to experiment with the use of different kinds of rigor. For example, \cite{chomsky55logicalsyntax} expressed doubts about the kinds of recursive definitions that \cite{barhillel:1954} was talking about, particularly in reference to \cite{carnap:1937}. Only later did recursive/computable devices of the form in  come to be used.\footnote{See \S~\ref{normcomp} for some discussion of the terms ``recursive'' and ``computable.''}$^,$\footnote{Only a few years later \cite{chomsky57ss} would come to recognize, in light of analysis of finite-state grammars expressed in the form of Markov machines, that ``If a grammar does not have recursive devices... it will be prohibitively complex. If it does have \textsl{recursive devices of some sort}, it will produce an infinitely many sentences'' (Chomsky 1957:24, italics mine).} 

\ex. General Rewrite Rule with Tree\\
S $\longrightarrow$ NP VP (\textbf{S})\label{rel} 
\Tree [.S [.NP ] [.VP [.(\textbf{S}) [.(NP) ] [.(VP) ]]]]              

\ex. Other Recursive Rewrite Rules\label{recstr}
\a. NP $\longrightarrow$ N (Det) (NP) \textbf{(PP)}
\b. PP $\longrightarrow$ P \textbf{(NP)} 

Such informal definitions by ``recursion'' proved to be problematic for describing general environments of linguistic phenomena---to the effect that the definitions in  were understood to be too strong (i.e., they produced structures that were either not empirically confirmed or simply ungrammatical). They were quickly generalized under the X-bar schema first proposed in \cite{chomsky70remarks} and later extended by \cite{jackendoff:1977}. 


\ex. X-bar generalization for constituents\label{xbar}
\a.X'' $\longrightarrow$ Specifier, X'
\b.X' $\longrightarrow$ X, (Complement)

Such that:\\
(i). X ranges over N, V, A, P, and S.\\ 
(ii). Endocentricity applies:\\ 
\hspace*{1cm}(ii$'$). X' is the head of X''.\\
\hspace*{1cm}(ii$''$). X is the head of X'.\\ 
(iii). Heads share categorial properties with their projections.\\ 
(iv). Complements are X'' categories.\\ 


The X-bar model was modified and streamlined by \cite{chomsky94bps} and \cite{chomsky95bps} in the form of a Bare Phrase Structure (BPS). Since that time, many proposals for a hyper-minimalist BPS have been made on the basis of both theoretical and empirical arguments; for example \cite{carnie:2000}, \cite{citko:2005}, \cite{collins:2001}, and \cite{jayaseelan:2008} to name a few. In BPS, the basic operation Merge \textsl{Selects} two syntactic constituents or objects and ``merges'' them together, giving as its output one constituent or object. The operation itself does not specify which of the two constituent labels projects---projection is assumed to be specified by the features of the objects being merged. For the purposes in this paper, no specification for projection will be given. It is reasonable to call this basic operation ``ambiguous,'' following \cite{boeckx08bare}, and I will denote its operation as $\alpha$Merge. 


\begin{definition}
$\{\{\alpha\}, \{\beta\}\} \stackrel{merge}{\longrightarrow} \{\Lambda, \{\alpha, \beta\}\}$ = $K$ = $\alpha$Merge \label{amerge}
\footnote{Where $\Lambda$ = exclusively the label either $\{\alpha\}$ or $\{\beta\}$. Also, where $\alpha$ and $\beta$ are two independent syntactic objects, $K$ is the new object output of Merge: so \{$\alpha$, $\beta\} \in K$ (are members of $K$), but \{\{$\alpha$, $\beta\}, \Lambda\} \subseteq K$ (both objects \textsl{and} the projected label are a subset equivalent to $K$); see \cite{chomsky95mp}. At first this might appear to violate \textsc{Inclusiveness}, but if one understands that label projection includes only the information contained in the constituents themselves, then a label projection of either $\alpha$ or $\beta$ seems only to be geometric extension of the constituent information. In essence, the output of merging two constituents becomes the input for a new (automatic) operation of projection---where the feature valuation between the two constituents decides which of them becomes the input to projection. This is a (tail) recursive procedure. It does not violate \textsc{Inclusiveness}. Additionally, it shows that---assuming Merge is a real operation in all human languages, i.e., is part of the language faculty (FL)---recursion is inherent to all human languages whether they show it explicitly or not (e.g., in clause embedding or possessives).}
\end{definition} 


The one thing that all, or most, current BPS models share in common is the general operation Merge defined as a physically computable operation. It has been hypothesized by \cite{hcf:2002} and \cite{fhc:2005} that Merge is operable in the domain of the Narrow Faculty of Language (FLN), specifically Narrow Syntax (NS), and is a unique property of human cognition.\footnote{It should be noted that $\alpha$Merge, as presented here, may be operable in non-human cognition. See \cite{jp:2005} for more on this, as well as arguments against \cite{hcf:2002}. Also note, $\alpha$Merge as presented here does not equate directly to adjunction defined in \cite{boeckx08bare}, Chapter 3, as a time-dependent insertion \{\{$\alpha$\}$_{t}$, \{$\beta$\}$_{t+1}$\} with an inherent temporal asymmetry. Although, there does not appear to be any major incompatibility; especially in light of the fact that my narrowed treatment of Merge is fairly abstract and mathematical in order to get at a group theory definition of symmetry from which more realistic and empirically based definitions of symmetry-breaking can be applied. See \S~\ref{gtsyn:sub} for more detail.} In tree form, $\alpha$Merge produces a mirror symmetric object, seen in Figure \ref{k}. 


\begin{figure}
\Tree [.$\Lambda$ $\alpha$ $\beta$ ]      = $K$
\caption{$\alpha$Merge tree produces object $K$}\label{k}
\end{figure}


\begin{figure}
\Tree [.$\Lambda$ $\alpha$ [ $\beta$ [ $\gamma$ $\delta$ ] ] ]   = $K'$
\caption{Iterated $\alpha$Merge tree produces object $K'$}\label{k'}
\end{figure}


With mirror-symmetric objects each side is the same in terms of their relative position and relation to the other side. Figure \ref{k} has a bilateral mirror-symmetry: rotating the object along a vertical axis 180$^{\circ}$ or 360$^{\circ}$ will not change it. Under these conditions it is understood that $\alpha$Merge is an idealized operation and $K$ is an idealized object. This operation can be iterated infinitely, where each label-node is ambiguous. A finite representation of the iteration is given in Figure \ref{k'}---notice that $K'$ is a compound of two mirror-symmetric trees but itself has no bilateral symmetry.\footnote{In fact, the tree in Figure \ref{k'} could be decomposed into \{$\alpha$, $\beta$\}, \{$\beta$, $\gamma$\} and \{$\gamma$, $\delta$\} with 3 ambiguous label slots, including the root node label, and 3 $\times$ 2 possible labels. Generalizing, for $n$ syntactic objects, there are ($n$--1) merge operations, or ($n$--1) label slots, and (($n$--1) $\times$ 2) possible labels. Compare this to the observation in \cite{medeiros:2008} of the relation between \textsl{string length} of $n$ and \textsl{depth} of ($n$--1).} 
Once the bilaterally symmetric object of Figure \ref{k} is repeated to produce a more complicated object it loses its simple mirror-reflection property. By these standards, even if one wanted to argue that $\alpha$Merge was necessary \textsl{and} sufficient to explain why human syntax is the way it is, some notion of symmetry-breaking must be introduced to account for strings containing more than two elements. For example, in Figure \ref{k'}, \{$\alpha$, $\beta$\} dominates the mirror-symmetric constituent \{$\gamma$, $\delta$\}, even though the label and/or category of \{$\gamma$, $\delta$\} is ambiguous. Interestingly, once $\alpha$Merge projects a label---no matter which label is, for the purposes of this paper, randomly projected from minimal/head level to maximal/phrase level---the number of phrase and head level constituents accumulated as the result of iterated Merge operations correlates with the Fibonacci pattern.  


\subsection{Fib(\emph{n}) levels and the X-ratio}\label{x}
\subsubsection{Rationale and justification}
The idealized objects produced in Figures \ref{k} and \ref{k'} are problematic because they do not allow \textsl{any} specification for label projection. The non-projective model of unrestricted Merge, or $\alpha$Merge as I am calling it, does not seem to produce any interesting patterns suggestive of natural law. But when one allows $\alpha$Merge to project a random choice of label the result is an interesting mathematical F pattern, as \cite{medeiros:2008} and \cite{soschen:2008} have shown; see also \cite{bcm:2006}, \cite{cm:2005}, \cite{idsardi:2008}, and \cite{ppuriagereka:2008}. However, Medeiros (2008:164) points out that 

\begin{quotation}It is tempting to see the appearance of the Fibonacci sequence in the X-bar pattern as being deeply significant in itself. But the X-bar schema is after all a very simple mathematical object, and there may be nothing particularly magical about the appearance of the Fibonacci sequence in the structures it generates. Their appearance in this domain could be no more of a surprise than their appearance in the family trees of bees, or in Fibonacci�s idealized rabbit populations, or in the number of metrical possibilities for a line of Sanskrit poetry, or any of the myriad situations these numbers describe. To put it another way, it could be that these properties are an accident of no �real� significance, or worse, merely a reflection of mathematical simplicity in linguists� description of language, rather than a property of language itself. Yet it is undeniable that patterns related to the Fibonacci sequence play an important role in nature, especially in optimal packing and optimal arboration.\end{quotation}

Additionally, \cite{livio:2002} provides analysis for many of the supposed F patterns that have been consciously encoded into art, architecture, and music, concluding that in many of the cases the investigators have been overzealous and mistaken---he even goes so far as to show how the dimensions of his television can be made to fit within the golden mean. Nonetheless, \cite{medeiros:2008} is right, F patterns are undeniable and their investigation in human syntax is significant---if approached soberly.

\subsubsection{The golden sequence (GS)}
The golden sequence (GS) is derived by a simple algorithm that correlates to the F sequence; it is given in Algorithm \ref{gsalgo}. A phrase structure equivalent is given in Algorithm \ref{psalgo}. The outputs of these algorithms are produced in Appendix A as Table \ref{gsop} and Table \ref{psop}---the latter of which generates the tree in Figure \ref{bigtree}; see \cite{uriagereka:1998} and \cite{ppuriagereka:2008} for linguistic applications of the GS. In the algorithms and their outputs no attention has been given to bar-level (X$'$) projections; only head (X$^0$) and phrase (XP) levels are of immediate concern---though \cite{medeiros:2008} and \cite{soschen:2008} include bar-levels. Lastly, the F level refers to the number of syntactic objects (heads and phrases), such that the number of syntactic objects follows the pattern of the F sequence. 


\begin{algorithm}\textsc{Golden Sequence:}\\
Start with 1, replace every 1 with 10, and every 0 with 1 in every iterated line.\label{gsalgo}  
\end{algorithm}

\begin{algorithm}\textsc{GS for Phrase Structure:}\\
Start with GS; each 1 = Phrase (XP = $\alpha$); each 0 = Head (X$^{0}$).\label{psalgo}
\end{algorithm}

Importantly, any specific parsing of elements by brackets in Table \ref{psop} is not determinate and can be changed. What is important is the fractional relation of heads to phrases, not how these head-phrase relations are parsed.\footnote{The F pattern is special because it codes a specific irrational value when two sequential numbers are put into a fraction such that the larger is the numerator; for example 13/8. As the numeral values get larger, and hence the fractions contain larger numeral values, their ratio gets closer to irrational limit. But this is an infinite limit, usually referred to as $\tau$, which has the value 1.618033\ldots.} In fact, the parsing, equivalent to the issue of determining the projecting head, is basically random in this approach. Also, it should be obvious that the name of the label does not matter and can be changed. 

The GS applied to phrase structure embedding gives similar results to \cite{medeiros:2008}, who maps explicit Fibonacci correlates of phrase structure in his approach. His examples (1-5, 2008:153-154) are numbered Phrase Strings 1 to 5 in Appendix B. These Medeiros strings have equivalents when an alternative parsing of the strings from Table \ref{psop} is applied---these latter equivalents are numbered Phrase Strings 6 to 10 in Appendix B. Consequently, one can see that the two approaches converge on equivalent results---with obvious and trivial differences in label names; see also Figure \ref{binaryamergetree}.

By bracketing the phrase string produced from Fibonacci level 34 ($F34$) differently than it is parsed in Table \ref{psop}, a fully balanced binary tree,like the one in \cite{medeiros:2008} and \cite{soschen:2008} can be derived. Figure \ref{medtree} is an example of a Medeiros tree and Figure \ref{medtreenospec} shows the same tree without specifiers. The Phrase Strings 11 and 12 represent the latter trees, respectively; 11 for Figure \ref{medtree} and 12 for Figure \ref{medtreenospec}. Phrase String 13 is derived from the GS algorithm for phrase structure and has been re-bracketed in order to correlate with the previous objects; it represents the embedding for Figure \ref{binaryamergetree}. One can easily see that by removing the X$'$ projections from 11, both 12 and 13 are equivalent---and clearly, so are their tree projections.\footnote{Potential problems are associated not with the actual re-bracketing, which is not a problem under the assumption of virtually random projections, but of the redistribution of constituents to form the balanced binary tree of Figure \ref{binaryamergetree}; compare the right-branching tree of Figure \ref{bigtree} derived from a straightforward sequential embedding of $F34$.}  

\begin{figure}
\Tree [.AP [.BP [.CP [.EP HP E$'$ ] [.C$'$ C$^{0}$ IP ]] [.B$'$ B$^{0}$ [.FP JP F$'$ ]]] [.A$'$ A$^{0}$ [.DP [.GP KP G$'$ ] [.D$'$ D$^{0}$ LP ]]]]
\caption{Medeiros tree}\label{medtree}
\end{figure}

\begin{figure}
\Tree [.AP [.BP [.CP [.EP HP ] [.C$^{0}$ IP ]] [.B$^{0}$ [.FP JP ]]] [.A$^{0}$ [.DP [.GP KP ] [.D$^{0}$ LP ]]]]
\caption{Medeiros tree with no Spec}\label{medtreenospec}
\end{figure}

\begin{figure}
\Tree [.AP [.CP [.DP [.BP EP ] [.D$^{0}$ GP ]] [.C$^{0}$ [.JP MP ]]] [.A$^{0}$ [.FP  [.OP SP ] [.F$^{0}$ UP ]]]]
\caption{Randomly projecting $\alpha$Merge tree adapted to Medeiros string}\label{binaryamergetree}
\end{figure}

\newpage
\begin{figure}[!htp]
\Tree [.AP A$^{0}$ [.BP  [.CP C$^{0}$ [.DP D$^{0}$ [.EP [.FP F$^{0}$ [.GP [.HP H$^{0}$ [.IP I$^{0}$ [.JP [.KP K$^{0}$ [.LP L$^{0}$ [.MP [.NP N$^{0}$ [.OP [.QP Q$^{0}$ [.RP R$^{0}$ [.SP [.TP T$^{0}$ [.UP [.ZP Z$^{0}$ ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] 
\caption{Random label projection of $\alpha$Merge}\label{bigtree}
\end{figure}
\newpage

In Figure \ref{bigtree} I produce a binary right-branching alternative to Medeiros' balanced binary tree.\footnote{Also work by Alana Soschen, for example \cite{soschen:2008}.} This right-branching alternative is based on a well-known property of F patterns called the Golden Sequence; for example \cite{livio:2002}, but especially \cite{uriagereka:1998}. It has the same F pattern for syntactic objects that derivations in \cite{medeiros:2008} do, and appears to display the same relations between maximal and minimal levels; though some redistribution of constiuents is needed to reformulate the tree in Figure \ref{bigtree} as the Medeiros equivalent in Figure \ref{binaryamergetree}. Redistributing constituents in such abstract and idealized models---where label projections are random and no interface conditions or features play a role---does not seem to pose a problem. The placement and order of constituents---basic c-command relations---depends on the order in which they are merged, but in the abstract models used here it does not matter if, for example, BP is merged before AP or I$^{0}$ is merged before DP. In other words, if the label projection is random then so is the operation \textsl{Select}, which selects items to be merged. What is really at issue here is the fractional relation between the number of constituents at phrase and head levels. Such fractional relations appear to reveal, at a high level of abstraction and idealization, a sequence or pattern that is commonly found in nature: the F pattern/sequence. 
What is interesting about Medeiros' treatment of the F pattern and its relation to idealized phrase structure patterns is that---abstracting even more---a Fib($n$) ratio between different comparative levels of phrase structure in a binary tree can be derived. For example, all maximal projections follow an initial sequence of the number pattern, Fib($n$), while terminal heads begin the pattern two levels 'down' at Fib($n$--2); (Medeiros 2008: 161). In other words, for maximal projections the sequence begins as shown in \ref{fxp}, but for terminal heads the sequence begins as shown in \ref{fxo} (again, unlike Medeiros, I ignore intermediate (X$'$) levels here and throughout). It does not matter how one derives the F pattern---whether by the golden sequence or other means---it still produces this ratio.  

\ex. Fib sequence for XP: [ 1, 1, 2, 3, 5, \ldots ]\label{fxp}

\ex. Fib sequence for X$^{0}$: [ 0, 0, 1, 1, 2, 3, 5, \ldots ]\label{fxo}	

By this relationship between levels, then, the ideal ratio between maximal and minimal projections is Fib($n : n$--2) at any horizontal level---or between sisters---in a binary tree. For example, the $F3$ string in Table \ref{psop} has one X$^{0}$ and two XPs (A$^{0}$, $\alpha$, and $\beta$) such that 2/1 = 2. Looking at $F34$, the  ratio of XP to X$^{0}$ is 21/13 = 1.615384\dots, an approximate to the limit ratio of the F sequence---usually denoted by ($\tau$ = 1.6180339\dots)---for which the ratio of maxima and minima of projections in binary phrase structure trees would get closer as the number of projections reached the limit of infinity within the F pattern. I will refer to the ratio Fib($n : n$--2), defined over maximal and minimal projections in phrase structure, as the X-Ratio; see \cite{cm:2005}, \cite{bcm:2006}, \cite{medeiros:2008}, and \cite{soschen:2008}. It is this ratio in Definition \ref{xratio} that leads to symmetry and Group Theory.

\begin{definition}
\textsc{X-Ratio:}\\ 
The ideal distributional ratio of maximal projections to minimal projections is\\ \mbox{Fib($n : n$--2)}.\label{xratio}
\end{definition}


\subsubsection{Symmetry in syntax}
Group Theory is the formal application of study for the investigation of symmetry in nature; see \cite{livio:2005} and \cite{stewart:2007} for popularized accounts and \cite{rosen:1995} and \cite{milne:2008} for formal introductions---also Chapter 10 in \cite{pmw:1990}. Once the X-ratio has been established, then the symmetry between maxima and minima of constituents, i.e., between phrases and heads, can be defined. By symmetry, I refer to the conjunctive notion defined informally by \cite{rosen:1995}. 


\begin{definition}
\textsc{Symmetry:}\\
(i) Possibility of change:  
It must be possible to perform a change, although the change does not actually have to be performed.
(ii) Immunity: 
Some aspect of the situation would remain unchanged, if the change were performed. 
\end{definition}

	
Under this view of symmetry, XP and X$^{0}$ are symmetric because the interchange between them of the arithmetical operation ($n\pm$2) on the Fibonacci sequence defining them does not change their underlying Fibonacci sequence---it only shifts the X-bar level at which the sequence begins. We might call this a \textsl{translational symmetry} such that there is an immunity to change dependent on a certain displacement or shift along a specific boundary. Such \textsl{translational} symmetries produce repeating patterns. Another way to describe the X-ratio would be as a \textsl{level symmetry}: XP and X$^{0}$ are immune to change---in terms of their Fibonacci patterns---relative to minimal or maximal level; i.e., relative to their head or phrase level. The difference, then, between maximal and minimal projections in binary trees is Fib($n\pm$2), which determines the X-bar level at which the F pattern begins, as in \ref{fxp} and \ref{fxo}. This is generally consistent with the empirical and theoretical claims of the hyper-minimalist BPS of, for example, \cite{carnie:2000}, where lexical features can be projected as XP or X$^{0}$ depending on specifics of the syntax of predication. 

However, the symmetry noted here is not the symmetry of $\alpha$Merge in \S~\ref{amerge:sub}. The X-ratio suggests a symmetry developed from constraints that determine the label projection; albiet the constraints used here are practically random. Needless to say, empirical data is more complicated and provides non-random constraints for the specification of projection. Importantly, the symmetries noted here and in \S~\ref{amerge:sub} do share one thing in common: they are both symmetries of syntactic objects and not of, say, syntactic rules of the sort discussed in \cite{boeckx08bare}.\footnote{For example, the ``Merge is source-independent'' rule which applies to a symmetry between External and Internal Merge such that under the operation \textsl{Select}, both types of Merge are immune to change; i.e., Merge is the same operation in all contexts. As a sidenote, \cite{boeckx08bare} highlights how this observation can be traced back to the \textsl{Structure Preservation Hypothesis} of Emonds. This seems like a significant advance in syntactic theory.} Thus, heads and phrases are symmetrical in an important way that differs from traditional notions of symmetry in syntax.\footnote{Such as subject-object asymmetries.} The current status of the notion of symmetry in syntax and prosody---based on empirical and theoretical observations---is suggestive of possible new directions and certainly justifies a consideration of GT and anything this particular formalism might offer as useful tools to the linguist. I suspect that what is developing in  current theoretical research is the groundwork for a potential discovery of a general principle (or group of principles) from which head and phrase levels can be derived. That is, head and phrase levels no longer need to be thought of as syntactic or linguistic primitives. Instead, they may be products of a general principle(s) of phrase structure based on some first principle(s) of natural law for efficient self-organization.\footnote{This kind of thinking meshes very well with \cite{baker:2003} and his attempt to derive category information from more basic syntactic structural relations. Baker defines two such relations in the following way: \textsl{X is a Verb (defined in 1) or a Noun (defined in 2) if and only if X is a lexical category and X} \begin{enumerate}\item has a specifier. \item bears a referential index, expressed as an ordered pair of integers.\end{enumerate}} These laws or first principles may be coupled with a combination of interface constraints and/or syntactic operations, such as Agree in \cite{boeckx08bare}, that essentially work to break the initial symmetry of ambiguously merged elements or elements that have the potential for head-phrase (X-ratio) ambiguity.\footnote{For example, clitics or nominal predicates.}

Of some immediate relevance here is the role of the formalisms of GT---the mathematical system used to study symmetry. It may not be necessary to incorporate much of the formal mechanism of GT, certainly not all of it, just as it is not necessary to incorporate most or all of Computability Theory simply because the notion of computability is crucial to theoretical syntax. However, having said this, it is at the very least interesting to flesh out some basic observations of symmetry in syntax in terms of the GT formalism; but I will not go into great detail.

\subsubsection{A conceptual rationalization for GT and symmetry} 
The benefit of GT is that it can model practically any kind of real world object (animate bodies, the Rubik's cube) and it can model abstract laws or rules. \cite{boeckx08bare} has shown how a rotational symmetry can be used to model symmetrical relations between projections and chains (pg. 46), see also \cite{uriagereka:1998}, and he has also modeled symmetry relations between specific linguistic rules to point out the essential fractal (i.e., self-repeating patterns at different levels of scale) nature of syntax (pg. 159); see also \cite{boeckxuri:2007} for discussion of fractals in nature and the Minimalist Program. \cite{medeiros:2008}, \cite{soschen:2008}, \cite{idsardi:2008}, and \cite{ppuriagereka:2008} have pointed out relevant patterns of F sequences in idealistic models of prosody, X-bar structure, and phase alternations. F sequences can be defined as a type of self-similar iteration that has a fractional limit of the value $\tau$ (= 1.618033\dots).\footnote{When one makes a fraction of any two sequential values in the F sequence, say [\dots, 5, 8, \dots] as 8/5 = 1.6 then take the next two values [\dots, 8, 13, \dots] as 13/8 = 1.625, one always gets closer to the limit value of $\tau$ but can never reach the limit because it is infinite.} This value is also the ratio repsonsible for making symmetrical spirals and other types of symmetrical objects.
If one takes seriously the notion that the human ability to acquire and use a linguistic system---a system unique in the animal kingdom---is really more of a reflex to a natural process based on a ``growth form,'' then the idea that symmetry (and more importantly symmetry-breaking) is a necessary property of such a ``growth form'' is a tantalizing possibility. Symmetry is one of the most profound properties of nature---and in fact, it is the slight deviation from symmetry that is so profound. It means that mulitple types of natural objects constructed of multiple kinds of material are at some abstract level following a similar blueprint. From spiral galaxies to human faces, this abstract blueprint is a defining characteristic of the laws of nature (not to mention fractals, which symmetrical forms can be related to). There is no reason to deny that, at an appropriate scale of measurement and abstraction, the ``growth form'' of the human language system must reveal the property of symmetry---and in fact does show deviance from symmetry in its real-world form.



%section:3
\section{Strong Minimalist Thesis}
This section covers three areas that form part of the backdrop for evaluating directions and formalisms that the MP can utilize---they are indirectly related to the ``three factors'' of \cite{chomsky05threefactors}. The first area deals with the biological nature of natural language. This means that although operations like Merge can be defined by rigorous formal and mathematical tools, Merge itself is not a formal object. Furthermore, the fact that an interesting mathematical pattern found throughout nature can also be found in prosodic and phrase structures does not mean that these structures are themselves mathematical or formal objects. (In fact, it does not even mean that the mathematical patterns are \textsl{inherent} to prosody or X-bar; the pattern may simply be a matter of our perception). The Strong Minimalist Thesis (SMT) proposes a general methodological backdrop for this and states, generally, that language is an optimal form meeting interface conditions. This implies that natural language is not a formal object, but an object of nature. The next area deals with the general notion that we expect, in lieu of strong contrary empirical evidence, operations in language to be in fact \textsl{operations that are computable}, i.e., specifiable by an algorithm that can be discovered or constructed. These computable operations must be physically tractable. That is, operations are bounded by human processing limits. Therefore, MP theories about syntax must meet certain natural limits. The third area is about the nature and concept of computablity itself and is intimately related to the second area. It deals with what linguists mean by the terms ``computable'' and ``recursive.'' The following subsections briefly outline these areas in order to give an explicit context for evaluating the usefulness of GT and formal notions of symmetry in syntax.  


\subsection{Merge is not a formal object}
Natural language is not a formal object. In this regard, neither is the operation Merge---though it, and its historical antecedents, are regularly defined through formal mathematical means. As early as \cite{chomsky55logicalsyntax} and \cite{chomsky57ss} the status of natural language as a formal object has been viewed critically within the context of twentieth-century advances in logical sytnax and semantics of the type found in \cite{carnap:1937}, \cite{ayer:1936}, \cite{riechenbach:1947}, \cite{quine:1953}, and \cite{tarski:1956}. This does not mean that useful approaches from formal theories cannot be used productively, only that natural language {probably} cannot be modeled \textsl{exhaustively} by any formal/mathematical system that yet exists---the implication here is that it is up to linguists to construct such a system from the formalisms that do exist, coupled with empirical evidence from natural langauges; for example \cite{langpostal:1984}, \cite{pmw:1990}, \cite{hornstein:1992}, \cite{prsmol:1993}, \cite{uriagereka:1998}, \cite{chmg:2000}, \cite{tessmol:2000}, \cite{bhj:2003}, \cite{niyogi:2006}, and of particular relevance here \cite{fortcm:toappear}. No matter the formal methods that are drawn on (usually set theory in semantics and syntax and probability theory in phonology and phonetics) it has generally been understood that, as \citep[7-8]{fortcm:toappear} state,

\begin{quotation}
As any abstract theoretical background, it is not reasonable to ask about
the reality of the operations and objects defined. For example, although the algorithm runs
through a time step indicator, such time step is only given for operational purposes and does not
imply---in our field of study---any temporal evolution. What is reasonable to ask is whether from
the defined mathematical framework we can derive the core properties that we observe in the
studied object (pp. 7-8). 
\end{quotation}

\citep[45]{chomsky55logicalsyntax} expressed a similar attitude in response to arguments by \cite{barhillel:1954} for the use of formal models in natural language---specifically the use of recursive definitions in a formal syntax of the type found in \cite{carnap:1937}:\footnote{Interestingly, as \cite{tomalin:2006} points out, Bar-Hillel's linguistic work served as the foundation for a specific approach to natural language, Combinatorial Categorical Grammar (CCG) widely used in computational linguistics.} 

\begin{quotation}
[t]he correct way to use the insights and techniques of logic is in formulating a general theory of linguistic structure. But this fact does not tell us what sort of systems form the subject matter for linguistics, or how the linguist may find it 	profitable to describe them. \textsl{To apply logic in constructing a clear and rigorous 	theory is different from expecting logic or any other formal system to be a model for linguistic behavior} (\textsl{italics mine}).
\end{quotation}

Additionally, a realistic theory of linguistics that takes natural language as a scientific object of inquiry must subject the brain to the known laws of biology, physics, and nature in general. In computational complexity theory, for example \cite{deutsch:1985}, \cite{deutsch:1997}, \cite{lloyd:2000}, \cite{lloyd:2006}, and also \cite{kauffman:1995}, all physical systems are understood to be computable and subject to the known laws of physics and computation, specifically entropy (the second law of thermodynamics) and universal computability (the Church-Turing Thesis). By normative scientific standards the brain, and by implication FL, cannot contradict laws of nature. I explicitly state this as a corollary of the \textsl{weak} minimalist thesis, which is an uncontroversial weakening of the strong minimalist thesis proposed in \cite{chomsky08onphases}.\footnote{The SMT is widely regarded as not being plausible---though possible---and is thus used as a heuristic standard. For this reason, it seems strange to derive a corollary from a heuristic. This is the motivation for putting forward a weakened version of the SMT.}

\newtheorem*{smt}{Strong Minimalist Thesis}
\begin{smt}
	Human language is an optimal solution to interface conditions that the faculty of language (FL) must satisfy.
\end{smt}

\begin{proposition}
\textbf{Weak Minimalist Thesis}: Human language is the natural product of linking sound (or gesture) and meaning.
\end{proposition}

\newtheorem*{coroll}{Corollary to Weak Minimalist Thesis}
\begin{coroll}
Human language is a product of the natural state of the organism-as-a-whole, and therefore, is subject to the same laws and constants that all of nature is.
\end{coroll}

Notice that the Corollary to the WMT does not say that human language, or more precisely FL, is explained by or reduced to all natural laws; nor does it say that all natural laws apply to FL---only that FL is subject to the laws and constants of nature (and by implication only a subset of natural laws). The Corollary is simply a compression of arguments found in many sources, including \cite{boeckxpp:2005}, \cite{chomsky86knowledge,chomsky95langnature,chomsky95mp}, \cite{ppuriagereka:2008}, \cite{uriagereka:1998} and many others past and present; see also \cite{formigari:2004} for historical consideration of issues relevant to the Corollary---particularly historical tensions between assigning natural language a ``spiritual'' or ``bestial'' cause relative to ``immaterial'' and ``material'' effects.

\subsubsection{Assumptions and the Church-Turing Principle} 
Assume the faculty of language (FL) is a finitely realizable physical system with an initial state $\Sigma$$_{UG}$ of Universal Grammar followed by subsequent computational states (C$_{1}$,\ldots, C$_{n}$). These states have a finite limit simply because they emerge from and are 'computed' by a finitely organic brain---a product of biological nature. But a finite realistic limit does not negate the notion of \textsl{discrete infinity.} The latter is a property of the computability of FL, not its real-world limit. In theory the initial state of FL, $\Sigma$$_{UG}$, has the computable potential of reaching infinity, $\Sigma$$_{\infty}$, but because of the inherent constraints of natural law applied to biological systems no real (i.e., physically computable) FL actually reaches the limit. Further assume, as any physical system that can be realized by computable functions, the FL can be modeled by a universal Turing machine, (\textsl{u}TM), as determined by the informal definition of the Church-Turing Thesis (CTT) given by \cite{deutsch:1985}. However, \cite{deutsch:1985} rejects the CTT as too vague and opts for a redefinition he calls the Church-Turing Principle (CTP). Assume the CTP applies to the MP.

\newtheorem*{ctthesis}{Church-Turing Thesis}
\begin{ctthesis}
Every function which would naturally be regarded as computable can be computed by the universal Turing machine.
\end{ctthesis}

\newtheorem*{ctprinciple}{Church-Turing Principle}
\begin{ctprinciple}
Every finitely realizable physical system can be perfectly simulated by a universal model computing machine operating by finite means.
\end{ctprinciple}

\cite[100]{deutsch:1985} elaborates:  
\begin{quotation}
I propose to reinterpret Turing's ``function which would naturally be regarded as computable'' as the functions which may in principle be computed by a real 	physical system. For it would surely be hard to regard a function 'naturally' as	computable if it could not be computed in Nature, and conversely. To this end I shall define the notion of 'perfect simulation'. A computing machine $M$ is capable of perfectly simulating a physical system $S$, under a given labelling of their inputs and outputs, if there exists a program $\pi$($S$) for $M$ that renders $M$ computationally equivalent to $S$ under that labelling. In other words, $\pi$($S$) converts $M$ into a 'black box' functionally indistinguishable from $S$
\end{quotation}

A syntax utilizing the formalization of a \textsl{u}TM in the linguistic domain has been extremely successful, especially if viewed from the perspective of the CTP. According to the MP, the physical system $S$ of the CTP is the brain---and there is an expectation that the computability of it be physically tractable, at least in terms of the computability of generating heirarchically nested structures that can be iterated (in)finitely to produce various forms (i.e., discrete infinity). For example, \cite{chomsky05threefactors} states the third factor of language design as ``Principles not specific to the faculty of language\ldots including principles of efficient computation'' (pg. 6). Here, efficiency is relative to the naturally finite biological limits of the human brain. There is a strong (conceptual) parallelism between Deutsch's redefining CTT as the physically bounded CTP and MP interests in the physical tractability of computable operations like Merge. Of course, David Deutsch employed his redefinition in terms of quantum phenomena and the computability of the universe---theoretical linguist's concerns for physical computability are dramatically narrower and more deterministic. However, this should not restrain linguistic interest in the physical computability of nature as long as one can see that such parallels here are, at first glance, simply analogous, conceptual, and intended to inspire---not direct a path of research.  

\subsection{Normative use of the term ``computability''}\label{normcomp}
Although the normative use of term ``computability'' may seem like a mundane topic, it in fact strikes to the heart of the issue of what linguists mean when they use the term ``recursive.''\footnote{One example of ``recursive'' can be found in \cite[181-82]{pmw:1990}, who show that a recursive definition can be gotten by considering \begin{quotation} the set $M$ of all mirror-image strings on \{$a$,$b$\}. A mirror-image string is one that can be divided into halves, the right half consisting of the same sequence of symbols as the left half but in the reverse order. For example, $aaaa$, $abba$, $babbab$, and $bbabbabb$\ldots. The following is a possible recursive definition of $M$. \begin{enumerate} \item $aa \in M$ \& $bb \in M$ \item ($\forall$$x$)($x \in M \rightarrow$ ($axa \in M$ \& $bxb \in M$)) \item $M$ contains nothing but those members it has by virtue of lines 1 and 2\\\end{enumerate}\end{quotation} Line 1 is the base, line 2 is the recursion step, and line 3 is the restriction. They go on to define the Principle of Mathematical Induction: \begin{quote} For any predicate $Q$, if the following statements [1 and 2] are both true of $Q$ then the following statement [3] is also true of $Q$: \begin{enumerate} \item $Q$0 \item ($\forall$$x$)($Qx \rightarrow Q$($S$($x$))) \item ($\forall$$x$)$Qx$\\ (pg. 196)\end{enumerate}\end{quote} Where $S$($x$) is the successor function---denotes the successor of ($x$)---and $Q$0 denotes the property $Q$ of zero. They note that the similarity between lines 1 and 2 in both demonstrations is ``readily apparent.'' But also say that the ``Principle of Mathematical Induction is not a definition, however, but a rule of inference to be applied to statements about the integers'' (pg. 196). Whatever similarities exist, recursion and induction as given here are not, arguably, equivalent in the strong sense. Furthermore, even if they are equivalent in the weakest sense of the term, the applications to which mathematical induction is relevant are not the kinds of applications relevant to the physically tractable computability concerns of the linguist.} \cite{soare:1996}, and more recently, \cite{soare:2007,soare:2008}, has pointed out a historical confusion in the use of the two terms that may lead to unfortunate misunderstandings relative to the field of study and the definitions assumed. He points out that there are both technical and now normative differences between the terms (i) ``(primitive) recursive function'' and (ii) ``recursively enumerable'' (r.e.). The latter term is closer to what linguists know as ``recursion,'' and as Soare suggests, there are important reasons for clearly delimiting  the difference in terminology. Substituting (ii) with the term ``computable'' and making it clearly different from ``recursion'' will have positive results in the sciences. \cite[34]{soare:1996} states that anyone using the term ``recursive'' always risks the chance that a mathematician or computer scientist will confuse it with induction: ``when we use 'recursive' to mean 'computable,' we are using it in a way that is not in any dictionary and which an educated scholar or scientist cannot reasonably be expected to know. Indeed, there is a danger that a computer scientist or mathematician might mistake it for 'inductive.''' I use the term Turing computable (TC) or ``computability'' to designate r.e., or the linguistic notion of ``recursion.'' Additionally, while discussing the fact that Emil Post's work focused directly on computably enumerable sets (which are usually called recursively enumerable sets) instead of computable functions---as found in the work of \cite{church:1936} and \cite{turing:1936}, although \cite{post:1936}---\cite[25]{soare:2008} says that
   
\begin{quotation}
concentration on c.e. [computably enumerable] sets rather than partial computable 	functions may be even more fundamental than the thesis of Church and Turing 	characterizing computable functions because\ldots often in higher computability theory it is more convenient to take the notion of a 	generalized c.e. set as basic and to derive generalized computable functions as 	those whose graphs are generalized computably enumerable.
\end{quotation} 

Recall that \cite[9]{chomsky65aspects} partly attributes the use of the term ``generative'' to the work of Emil Post (Chomsky is defending the normative and technical use of ``generative''). A relevant quote can be taken from \cite[286]{post:1944}, also in \cite{soare:2008}: ``every \textsl{generated set} is effectively enumerable, every effectively enumerable set of positive integers is recursively enumerable'' (\textsl{italics mine}). Chomsky attributes part of the motivation for the term ``generative'' to Post's work on---what Soare argues should now be called---computably enumerable sets. That is, ``generative'' seems to refer more to functions defined by (or sets enumerated by) Turing machines, algorithms, and concepts generally associated with computation than to definition by induction, general recursive functions defined by Herbrand-G\"odel, or fixed points in the Kleene Recursion Theorem. These latter notions do not generally appear to be utilized by linguists and are not familiar as part of the formal methods used to investigate I-language. Of the latter terms specific to recursion there is one possible exception: an apparent equivalence between \textbf{definition by induction} and \textbf{definition by recursion}, but this equivalence is only arguably apparent---though I give no demonstration. Additionally, even if a significantly relevant equivalence between definition by recursion and definition by induction is real, this does not change the fact that linguists do not use the latter because it is in fact only useful for investigating the natural numbers $\mathbb{N}$ and arithmetic in general---not natural language syntax and semantics. Nonetheless, the concept of definition by recursion has been used widely; example  in this paper, \cite{pmw:1990}, and \cite{chmg:2000}.\footnote{\cite{chmg:2000} state for the bracket [$_{A}$B C] that \begin{quotation} We have to specify the value of the tree whose root is $A$ in terms of the values of the subtrees rooted in $B$ and $C$. This means that the semantic value for the terminal string dominated by $A$ is determined in terms of the values of the substrings dominated by $B$ and $C$ and the way these substrings are put together. If we do this for every syntactic rule in the grammar, we can interpret any tree admitted by it. A definition of this kind (with a finite number of base clauses and a finite number of clauses that build on the base clauses) is called \textsl{recursive} (pg. 76). \end{quotation}} But this kind of definition, used within the context of linguistics, is useful only in order to construct algorithms that are (finitely) computably efficient and physically tractable within the general domain of human cognition---and in terms of the operation Merge, within the domain of narrow syntax (NS) and the narrow faculty of language (FLN).
Lastly, \cite[29]{soare:1996} proposes that a historically normative convention has largely directed the use, and sometimes misuse, of the term ``recursive.''
	\begin{quotation} 
	The Recursion Convention has brought `recursive' to have at least four different 	meanings.... This leads to some ambiguity. When a speaker uses the word 	`recursive' before a general audience, does he mean `defined by induction,' `related to fixed points and reflexive program calls,' or does he mean 	`computable?' [\ldots] Worse still, the Convention leads to imprecise thinking about 	the basic concepts of the subject; the term `recursion' is often used when the 	concept of `computability' is meant. (By the term `recursive function' does the 	writer mean `inductively defined function' or `computable function?') 	Furthermore, ambiguous and little recognized terms and imprecise thinking lead 	to poor communication both within the subject and to outsiders, which leads to 	isolation and lack of progress within the subject, since progress in science 	depends on the collaboration of many minds.
  \end{quotation}

Group Theory is not so much explicitly employed in computability theory but set theory is, and since GT presupposes some set theory it is an important, albeit minor, issue to note that research into how the operation Merge works is research into its computable nature---\textsl{\textsc{not}} its recursive nature. As with the parallel to the CTP, linguistic science also has parallels with computability theory and shares the latter's distinction between (i) an operation $M$ being computable by finite means that are describable by algorithms---for example \cite{fortcm:toappear} are developing a nesting machine based on a set-theoretically defined algorithm within the general theory of order---or trying to determine computable operations for certain functions of $M$, or even generating sets of objects that are countable by $M$, and (ii) an operation $M'$ that is defined by induction or uses primitive recursive functions. It is within the general backdrop of computability, not recursive function theory, that any formalism of GT will be successful in theoretical syntax. An analogy can be made to the linguistic use of Turing machines (TM): theoretical syntacticians are more concerned with a specific type of universal TM\footnote{This is the Turing \textsl{automatic}-Machine, compared to a stochastic/probabilistic TM, or even a quantum TM that \cite{deutsch:1985} formalized.} as it relates to the operation Merge within the larger backdrop of the concern for algorithms that are both physically tractable and computably efficient.

\subsection{GT and syntax}\label{gtsyn:sub}
Group Theory, or GT, presupposes some knowledge of set theory because, technically, any group is also a set. That is, any collection of objects, including a set or collection of sets, can be a group if it can be defined by Definition \ref{g}; the following is loosely based on \cite{rosen:1995}.\footnote{Notice and interesting parallel with the criterion given below and some examples from \cite{boeckx08bare} when discussing the ``symmetry problem'' for Merge (pp. 79-80). In discussing the empirical inadequacy of, what I am calling $\alpha$Merge, he stresses that it cannot produce \textsl{asymmetric} relations such as precedence ($a \prec b \neq b \prec a$), and also prosody defined over prominence ([[ A, B] C ] $\neq$ [ A [ B, C ]]). These are equivalent to Crierion 1 and Criterion 2, respectively.}

\begin{definition}
\textsc{Group:}\\
$G$ is a group iff $G$, under a law of composition, meets Criterion 1 to 4.\label{g} 
\end{definition}

\begin{criterion}
\textsl{Closure:}\\ For all $a$, $b$ : $a$, $b \in G$, then $ab$, $ba \in G$.
\end{criterion}

\begin{criterion}
\textsl{Associativity:}\\ For all $a$, $b$, $c$ : $a$, $b$, $c \in G$, then $a$($bc$) = ($ab$)$c$.
\end{criterion}

\begin{criterion}
\textsl{Existence of Identity:}\\ $G$ contains identity element $e$ : $ae = ea = a$.
\end{criterion}

\begin{criterion}
\textsl{Existence of Inverses:}\\ For every $a \in G$, then $a^{-1} \in G$ : $aa^{-1} = a^{-1}a = e$.
\end{criterion}

A ``law of composition'' is broadly defined as any procedure that combines any two elements in any way (i.e., a binary operation). This can include the combination of functions $f(n)$,  relations $R$, integer values in addition or mulitplication ($t + y$, $t \times y$), or the combination of collections of objects that are subsets of a set $s \in S$. Compositions also include permutations, rotations, and translations by displacement of a certain distance along a certain line. (For group theory, see especially Chapter 10 in \cite{pmw:1990}; also \cite{milne:2008}, section 39 in \cite{kleene:1967}; also \cite{korfhage:1974}, \cite{dornhoffhohn:1978}---or any abstract or modern algebra book; also \cite{livio:2005} and \cite{stewart:2007} for historical and popularized introductions to symmetry and groups.) The genius of GT is that it is so broadly defined that it can describe a wide variety of concrete or abstract objects, but is rigorous enough to derive very precise relations between those objects. I begin with a general notion for a law of composition.

\newtheorem*{comp}{Law of Composition}
\begin{comp}
\textsc(informal):\\
Any two possible elements, $a$, $b$, may be combined in any possible way, where `$\circ$' is any combination, iff $a \circ b$ equals a set $S$.
\end{comp}

I now briefly sketch how $\alpha$Merge meets the criteria in Definition \ref{g}, giving a proof. I then discuss some issues arising from the conjecture that syntactic operations can be defined using criteria from GT.

\newtheorem{gmerge}{Conjecture}
\begin{gmerge}
The operation $\alpha$Merge forms a group.
\end{gmerge}


\begin{proof}
Assume no other operation Merge and no interface constraints, then the following conditions hold:

\textsl{\textbf{Closure:}} If $\{\{\alpha\}, \{\beta\}\} \stackrel{merge}{\longrightarrow} \{\Lambda, \{\alpha, \beta\}\} = K$, (where $\Lambda\ = \alpha$ or $\beta$), then \{$\alpha$, $\beta\} \in K$ and \{$\beta$, $\alpha\} \in K$. This satisfies closure and is analogous to the c-command relation $R$ in the following way: if $\alpha R \beta$, then $\beta R \alpha$.

\textsl{\textbf{Associativity:}} If [$\Lambda$ $\alpha$ [ $\beta$ [ $\gamma$ $\delta$ ] ] ] = $K'$ and all label projections are ambiguous, then $K'$ equals the decomposition \{$\alpha$, $\beta$\}, \{$\beta$, $\gamma$\} and \{$\gamma$, $\delta$\}. By associativity, then, $K'$ can also be decomposed into $\alpha$\{$\beta$, $\gamma$\} and  \{$\alpha$, $\beta$\}$\gamma$. This is analogous to the c-command relation $R$ in following way: if, for example,  $\alpha R$ \{$\beta$, $\gamma$\}, then  \{$\alpha$, $\beta$\} $R \gamma$. 

\textsl{\textbf{Identity:}} By analogy, where rotation by 360$^\circ$ is equivalent to the absence of rotation---which is equivalent to an identity for rotation---the absence of the operation $\alpha$Merge occurring is equivalent to the identity function. Then, by stipulation $\exists$($\neg\alpha$Merge), and by identity, \{$\alpha$\}, \{$Y$\}$\stackrel{\neg merge}{\longrightarrow} \{\alpha\}$, where $Y$ is any constituent or set of constituents.

\textsl{\textbf{Inverse:}} By stipulation, if $\exists \{\alpha\}$ then $\exists \{\alpha^{-1}\}$. 

\end{proof}

The last stipulation for the existence of inverses is the biggest problem. Compared to the stipulation for the existence of identity---which is simply the non-occurrence of the operation $\alpha$Merge---the existence of inverses is harder to rationalize. Because it is specific for actual constituents in the derivation it might be interpreted as saying that lexical or functional items have inverses---or that lexical entries must encode some kind of information for its inverse. This is not so; in fact, the inverse can be part of the mechanism of a numeration $N$ and not of actual constituents. For now, I assume the legitimacy of this reasoning  based on the following line of argument. Assume a numeration $N$ consists of a finite set of elements that must all be selected, merged, and meet further requirements specific to Case, Phase, Agree, and other mechanisms or features belonging to a real-world syntactic derivation. Within a particular $N$, all elements consist of a set of pairs (LI, $i$) where LI is the lexical item and $i$ is the index, \cite{chomsky95mp}. Once the index $i$ is reduced to zero, such that it can no longer be selected to merge by External Merge (an extension \textsl{or} equivalent of $\alpha$Merge), then it seems rational to conclude that such a zero index specification may act as the inverse for whatever LI it was indexing. For items with an integer value greater than 1 for its index---so that it must be selected mulitple times in order to reduce to zero---the inverse is formed from its correlational index. That is, (DP$_{1}$, DP$_{2}$, \ldots, DP$_{n}$) correlates to (DP$_{-1}$, DP$_{-2}$, \ldots, DP$_{-n}$). In this way no problem arises for having to propose an inverse for lexical items. Instead, the inverse belongs to the abstract structure of the numeration. In other words, inverses are constructed from the indices of $N$ so that an item in the numeration with (LI, $i$) as its pair will also have ($i^{-1}$) and would look like this: (LI, $ii^{-1}$). This is an explicit representation of the idea that an index \textsl{must} reduce to zero (actually its inverse). Under this interpretation then, indices do not actually reduce to a numerical zero, but instead, they reduce to their prespecified inverse. Consequently, since the index is really just a placeholder within the numeration and does not belong to the feature content of LI's, the inverse of this placeholder is also not specified as a feature of LI, but of the abstract structure of $N$ generally.

What GT has to offer syntax generally deals with the nature of derivations. For example, trees derived by iterations of $\alpha$Merge, such as Figure \ref{k'}, can be shown to form an Abelian group because any element can be \textsl{commuted} with any other element. This is certainly not the case with real-world trees derived from a more complicated and natural operation Merge. However, some elements in trees can commute with each other. Or rather, some \textsl{positions} can commute based on feature specification and probe-goal relations. These form chains $CH$, which, under GT considerations, might be Abelian subgroups $g$ belonging to a non-Abelian group $G$. Certainly, in GT these kinds of things are possible. And if the group $G$ is redefined as the derivation $D$, then a chain $CH$ of $D$ may possibly form such a subgroup relation.

Applications from a GT perspective, namely one that defines $\alpha$Merge by Criterion 1 through 4, remain to prove their worth---but the possibility of rigor for defining a base notion of symmetry from which Merge originates and acts as \textsl{symmetry breaking agent} is promising. Such possibilities for syntax make certain aspects of GT a welcome convenience. It may provide formal rigor in establishing interesting relations of general symmetry, and especially symmetry-breaking, in natural language.

%section 4:-------------------------------------------------
\section*{Conclusion}
This paper has given a very brief historical background to the operation Merge, showing how its current conception is the result of a narrowed focus on the role of the logical tool of \textsl{definition by recursion} for constituents. Early transformational-generative grammars specified unique recursive definitions for constituents, example , while the X-bar model that followed generalized this definition in the form of a variable defined over lexical and functional categories, example \ref{xbar}. The current MP notion of Merge is that it is an abstract operation ocurring only at local intervals, blind to future operations, and based strongly on a Bare Phrase model of the X-bar type. Given this context, \cite{medeiros:2008} has constructed an idealized balanced binary branching tree that exhibits a common mathematical pattern found in nature, the Fibonacci pattern, that corelates the number of X-bar objects/levels with the mathematical pattern that defines a Fibonacci level in a downward sequence; see also \cite{uriagereka:1998}, \cite{cm:2005}, \cite{bcm:2006}, \cite{idsardi:2008}, \cite{soschen:2008}, and \cite{ppuriagereka:2008} for use of the F-pattern. I show that if one follows the algorithm of the Golden Sequence, which derives the F-pattern, and correlates the substitution criteria (1, and 0) with XP and X$^{0}$ levels, a right-branching equivalent to \cite{medeiros:2008} and \cite{soschen:2008} results. By abstracting further, and looking only at head and phrase levels, a \textsl{translational symmetry} can be seen between the two levels and defined as a ratio between their co-ordinate F-levels as F($n : n-2$); also called the X-ratio. This is one measure of symmetry that can be found in syntax, but there are other non-trivial kinds, as \cite{boeckx08bare} shows. 

This paper also discusses issues relevent to methodological concerns stemming from the Strong Minimalist Thesis as they relate generally to the potential use of formal tools in syntax, namely tools from Group Theory. These issues deal mainly with (i) the biological nature of natural langauge, (ii) that operations in natural language are in fact \textsl{operations that are computable}, and (iii) the nature and concept of computability itself. An argument for the biological nature of Merge is given, constrasted to a view of Merge---and human language---as a formal object. Additionally, a weakened version of the SMT is proposed and a corrollary derived from it---both focusing on the view that human language is at least partially a product of nature. Working from the ``Corollary to the Weak Minimalist Thesis,'' I highlight some parallels between the MP and research into concepts of the computability of natural systems---though such parallels are meant to be taken in a conceptual and analogous light. Most useful in this context is the Church-Turing Principle, which is a naturalistic redefinition of the Church-Turing Thesis, given in \cite{deutsch:1985}. Following this, I focus on arguments made by the mathematician and computability theorist \cite{soare:1996,soare:2007,soare:2008} about the normative use of the term ``computability,'' in contrast to the sometimes misused term ``recursive,'' as it applies to the linguistic field. It seems reasonable that, given linguists are more concerned with algorithms computable through discretely infinite means and processed by finite machines (brains), the use of the term ``recursive'' should be dropped in favor of ``computability;'' the latter of which already implies the kind of recursivity linguists are interested in.

Finally, the fact that various kinds of non-trivial natural symmetries can be defined in syntax is strong support for the investigation of the formal study of symmetry, Group Theory. Additionally, given the far-reaching goal for more rigorous methods of measurement and analysis in syntax, it is reasonable to look at Group Theory as possibly providing some useful tools to the theoretical linguist. For this reason, I provide a group theoretic definition of the idealized operation ambiguous Merge, $\alpha$Merge, and briefly discuss some implications of this definition---specifically that a numeration $N$ may have an inverse $i^{-1}$ for any index $i$ that exists. Now, instead of an index reducing to zero, it simply reduces to its inverse. There also appear to be, on the face of it, possible uses of Group Theory in defining, measuring, or analyzing the behavior of Chains and their formal relations to derivations. 
   
\newpage

%Appendix--------------------------------------------------------
\section*{Appendix A}\label{gsappend}
\begin{table}[!hbp]\caption{Golden sequence output}\label{gsop}
\begin{tabular}[c]{l @{} r}
Binary & F(\textsl{n}) level\\
\hline
1 & $F1$\\
10 & $F2$\\
101 & $F3$\\
10110 & $F5$\\
10110101 & $F8$\\
1011010110110 & $F13$\\
101101011011010110101 & $F21$\\
1011010110110101101011011010110110 & $F34$\\
\vdots & \vdots
\end{tabular}
\end{table}


\begin{table}[!hbp]\caption{Phrase structure output}\label{psop}
\begin{tabular}{l@{}l}
F(\textsl{n}) level Phrase embedding\\
\hline

$\mathbf{F1}$ $\Rightarrow$ $\alpha$(= XP) \\

$\mathbf{F2}$  $\Rightarrow$ [ $\alpha$ A$^{0}$ ]\\

$\mathbf{F3}$ $\Rightarrow$ [ $\alpha$ A$^{0}$ [ $\beta$ ] ] \\

$\mathbf{F5}$ $\Rightarrow$ [$\alpha$ A$^{0}$[ $\beta$ [ $\gamma$ B$^{0}$ ] ] ] \\

$\mathbf{F8}$ $\Rightarrow$ [$\alpha$ A$^{0}$[ $\beta$ [ $\gamma$ B$^{0}$ [ $\delta$ C$^{0}$ [ $\epsilon$ ] ] ] ] ] \\

$\mathbf{F13}$ $\Rightarrow$ [$\alpha$ A$^{0}$ [ $\beta$ [ $\gamma$ B$^{0}$ [ $\delta$ C$^{0}$ [ $\epsilon$ [ $\zeta$ D$^{0}$ [ $\eta$ [ $\theta$ E$^{0}$ ] ] ] ] ] ] ] ] \\

$\mathbf{F21}$ $\Rightarrow$ [$\alpha$ A$^{0}$ [ $\beta$ [ $\gamma$ B$^{0}$ [ $\delta$ C$^{0}$ [ $\epsilon$ [ $\zeta$ D$^{0}$ [ $\eta$ [ $\theta$ E$^{0}$ [ $\iota$ F$^{0}$ [ $\kappa$ [ $\lambda$ G$^{0}$ [ $\mu$ H$^{0}$ [ $\nu$ \\

\hspace*{0.3cm}] ] ] ] ] ] ] ] ] ] ] ] ] \\

$\mathbf{F34}$ $\Rightarrow$ [$\alpha$ A$^{0}$ [ $\beta$  [ $\gamma$ B$^{0}$ [ $\delta$ C$^{0}$ [ $\epsilon$ [ $\zeta$ D$^{0}$ [ $\eta$ [ $\theta$ E$^{0}$ [ $\iota$ F$^{0}$ [ $\kappa$ [ $\lambda$ G$^{0}$ [ $\mu$ H$^{0}$ [ $\nu$ \\

\hspace*{0.3cm}[ $\xi$ I$^{0}$ [ $\pi$ [ $\rho$ J$^{0}$ [ $\sigma$ K$^{0}$ [ $\tau$ [ $\upsilon$ L$^{0}$ [ $\phi$ [ $\chi$ M$^{0}$ ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ]\\
\end{tabular}
\end{table}

A brief note about the use of lowercase Greek letters and uppercaase Roman letters: the different letters used in Table \ref{psop} run sequential to their relative level; e.g., Greek letters run their sequence at the phrase level, and Roman letters theirs on the head level.\newpage


\section*{Appendix B}\label{psappend}%

\begin{phrase string}{}
[$\alpha$ [ $\beta$ \ldots [ $\gamma$ [ X$^{0}$ $\delta$ ] ] \ldots ] ] 
\end{phrase string}

\begin{phrase string}{} 
[X$^{0}$ $\alpha$ ]
\end{phrase string}

\begin{phrase string}{}
[$\alpha$ [ X$^{0}$ $\beta$ ] ]
\end{phrase string}

\begin{phrase string}{} 
[$\alpha$ [ $\beta$ [ X$^{0}$ $\gamma$ ] ] ]
\end{phrase string}

\begin{phrase string}{}
[X$^{0}$ [ $\alpha \beta$ ] ] 
\end{phrase string}

\begin{phrase string}{} 
[$\beta$ [ $\gamma$ \ldots [ $\delta$ [ C$^{0}$ $\epsilon$ ] ] \ldots ]]	
\end{phrase string}
 
\begin{phrase string}{} 
[A$^{0}$ $\beta$ ] 																											  
\end{phrase string}

\begin{phrase string}{}
[$\alpha$ [ A$^{0}$ $\beta$ ] ]																					  
\end{phrase string}

\begin{phrase string}{}
[ $\beta$ [ $\gamma$ [B$^{0}$ $\delta$ ] ] ]																
\end{phrase string}

\begin{phrase string}{}
[A$^{0}$ [ $\beta \gamma$ ] ]																						      
\end{phrase string}

\begin{phrase string}{}
[AP [BP [CP [EP HP E$'$ ] [C$'$ C$^{0}$ IP ] ] [B$'$ b$^{0}$ [FP JP F$'$ ]]] [A$'$ A$^{0}$ [DP [GP KP G$'$ ]\\

[D$'$ D$^{0}$ LP ]]]]
\end{phrase string}

\begin{phrase string}{}
[AP [BP [CP [EP HP ][C$^{0}$ IP ]] [B$^{0}$ [FP JP ]]] [A$^{0}$[ DP[ GP KP][D$^{0}$ LP]]]]
\end{phrase string}

\begin{phrase string}{}
[AP [CP [DP [BP EP ] [D$^{0}$ GP ]] [C$^{0}$ [JP MP ]]] [A$^{0}$ [FP [OP SP ] [F$^{0}$ UP ]]]]
\end{phrase string}
\newpage

\newpage


%Bibliography------------------------------------------------------
\bibliography{myrefs}



\end{document}

